import os
from openai import OpenAI
from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException, Request

# åˆå§‹åŒ– FastAPI router
router = APIRouter()

# è¼‰å…¥ .env æª”
load_dotenv()

# å¾ç’°å¢ƒè®Šæ•¸è®€å–é‡‘é‘°èˆ‡çµ„ç¹”ã€å°ˆæ¡ˆ ID
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORG_ID = os.getenv("OPENAI_ORG_ID")
OPENAI_PROJECT_ID = os.getenv("OPENAI_PROJECT_ID")

def get_openai_client() -> OpenAI:
    if not OPENAI_API_KEY:
        raise ValueError("âŒ ç¼ºå°‘ OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")

    if OPENAI_ORG_ID:
        client = OpenAI(api_key=OPENAI_API_KEY, organization=OPENAI_ORG_ID)
    else:
        client = OpenAI(api_key=OPENAI_API_KEY)

    print("âœ… OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
    return client

async def generate_response(client: OpenAI, messages: list, model: str = "gpt-4o-mini", max_tokens: int = 1000, temperature: float = 0.8) -> str:
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        reply = response.choices[0].message.content if response.choices else ""
        print(f"ğŸ’¬ AI å›æ‡‰: {reply[:60]}{'...' if len(reply) > 60 else ''}")
        return reply
    except Exception as e:
        print(f"âŒ OpenAI API éŒ¯èª¤: {e}")
        raise

# âœ… æ–°å¢ä¸€å€‹ POST API è·¯ç”±ï¼š/api/openai/chat
@router.post("/openai/chat")
async def chat_with_openai(request: Request):
    try:
        data = await request.json()
        messages = data.get("messages", [])
        model = data.get("model", "gpt-4o-mini")
        max_tokens = data.get("max_tokens", 1000)
        temperature = data.get("temperature", 0.8)

        client = get_openai_client()
        reply = await generate_response(client, messages, model, max_tokens, temperature)
        return {"response": reply}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
